# Ejecutar en terminal:
# python3 clase-03/04_Rag_mozo.py


"""
Este script implementa un agente conversacional que simula ser un mozo virtual
llamado "Bruno" para el restaurante "La Delicia". Utiliza LangGraph y un sistema RAG.

Funcionalidades principales:
1.  Carga de un menÃº detallado y datos del restaurante como documentos.
2.  CreaciÃ³n de una base de datos vectorial (Chroma) persistente con la informaciÃ³n
    del menÃº para realizar consultas semÃ¡nticas.
3.  DefiniciÃ³n de un LLM (Gemini 1.5 Flash) con el rol de un mozo.
4.  Herramientas:
    - Un 'retriever' para buscar en el menÃº.
    - Una herramienta 'off_topic' para manejar preguntas no relacionadas.
5.  ConstrucciÃ³n de un grafo con LangGraph para orquestar la conversaciÃ³n y el uso de herramientas (patrÃ³n ReAct).
6.  Un bucle interactivo para chatear con "Bruno".
"""
import os
from typing import Sequence, Annotated, TypedDict, Literal

# Carga de variables de entorno
from dotenv import load_dotenv

# Componentes de LangChain
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.tools.retriever import create_retriever_tool
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage

# Componentes especÃ­ficos de Google
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

from langchain_chroma import Chroma

# Componentes de LangGraph
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

# --- 1. CONFIGURACIÃ“N INICIAL ---

def setup_environment():
    """Carga las variables de entorno desde el archivo .env."""
    load_dotenv()
    if not os.getenv("GEMINI_API_KEY"):
        raise ValueError("La variable de entorno GEMINI_API_KEY no estÃ¡ definida.")
    print("âœ… Variables de entorno cargadas correctamente.")

# --- 2. CARGA DE DATOS DEL RESTAURANTE (MENÃš) ---

def load_documents() -> list[Document]:
    """Carga los documentos que representan el menÃº y la informaciÃ³n del restaurante."""
    

    # Un solo documento con todo el menÃº
    menu_text = """
    Aperitivos:
    - Bruschetta ClÃ¡sica: Pan tostado con tomates frescos, ajo, albahaca y aceite de oliva. Precio: $8. Ingredientes: pan, tomate, ajo, albahaca, aceite de oliva.
    - Tabla de Quesos y Fiambres: SelecciÃ³n de quesos locales e importados con jamÃ³n serrano y salame. Precio: $15. Ingredientes: quesos variados, jamÃ³n serrano, salame.

    Platos Principales:
    - Lomo a la Pimienta: MedallÃ³n de lomo de 250g con una cremosa salsa de pimienta negra, acompaÃ±ado de purÃ© de papas. Precio: $28. Ingredientes: lomo, pimienta, crema, purÃ© de papas.
    - SalmÃ³n a la Parrilla con Vegetales: Filete de salmÃ³n fresco grillado con una guarniciÃ³n de vegetales de estaciÃ³n. Precio: $25. Ingredientes: salmÃ³n, vegetales de estaciÃ³n.
    - Risotto de Hongos: Arroz arbÃ³reo cremoso con una mezcla de hongos silvestres y aceite de trufa. Es un plato vegetariano. Precio: $22. Ingredientes: arroz arbÃ³reo, hongos, aceite de trufa, queso parmesano.

    Postres:
    - TiramisÃº: ClÃ¡sico postre italiano con capas de bizcocho, cafÃ©, mascarpone y cacao. Precio: $9. Ingredientes: bizcocho, cafÃ©, queso mascarpone, cacao.
    - VolcÃ¡n de Chocolate: Bizcocho tibio de chocolate con centro lÃ­quido, servido con helado de vainilla. Precio: $10. Ingredientes: chocolate, helado de vainilla.

    Bebidas:
    - Vino Malbec (copa): Vino tinto de la casa. Precio: $7.
    - Limonada con Menta y Jengibre: Bebida refrescante sin alcohol. Precio: $5.
    """
    menu_docs = [
        Document(
            page_content=menu_text,
            metadata={"source": "menu.txt"}
        )
    ]
    print(f"ğŸ“„ MenÃº unificado en un solo documento.")


    # Un solo documento con toda la informaciÃ³n del negocio
    negocio_info = """
    El restaurante La Delicia es propiedad de Antonio Rossi, un chef de renombre con mÃ¡s de 20 aÃ±os de experiencia en cocina italiana.
    UbicaciÃ³n: Av. Italia 1234, San Carlos de Bariloche, RÃ­o Negro, Argentina.
    La Delicia abre de martes a domingo. Horario: 12 PM â€“ 4 PM para el almuerzo, y 8 PM â€“ 11 PM para la cena. Lunes cerrado.
    TelÃ©fono: +54 294 412-3456
    Email: reservas@ladelicia.com.ar
    Especialidad: Cocina italiana tradicional y platos internacionales.
    Ambiente: Familiar y acogedor, ideal para reuniones y celebraciones.
    Capacidad: 60 cubiertos.
    Se aceptan reservas y pagos con tarjeta.
    """
    info_docs = [
        Document(
            page_content=negocio_info,
            metadata={"source": "info.txt"}
        )
    ]
    print("ğŸ“„ InformaciÃ³n del negocio unificada en un solo documento.")

    return menu_docs + info_docs

# --- 3. CREACIÃ“N DEL VECTORSTORE PERSISTENTE ---

def create_or_load_vectorstore(documents: list[Document], embedding_model) -> Chroma:
    """Divide los documentos y crea o carga una base de datos vectorial Chroma persistente."""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    splits = text_splitter.split_documents(documents)
    
    vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)
        
    print("âœ… Vectorstore listo.")
    return vectorstore

# --- 4. DEFINICIÃ“N DE HERRAMIENTAS ---

@tool
def off_topic_tool():
    """
    Se activa cuando el usuario pregunta algo no relacionado con el restaurante,
    el menÃº, los precios o los horarios.
    """
    return "Disculpe, como mozo virtual de 'La Delicia', solo puedo responder preguntas sobre nuestro menÃº y servicios. Â¿Le gustarÃ­a saber algo sobre nuestros platos?"

def define_tools(vectorstore: Chroma) -> list:
    """Define las herramientas que el agente mozo podrÃ¡ utilizar."""
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # Aumentamos k para mÃ¡s contexto
    
    retriever_tool = create_retriever_tool(
        retriever,
        name="consultar_menu_y_horarios",
        description="Busca y recupera informaciÃ³n sobre los platos del menÃº, ingredientes, precios, opciones vegetarianas, y tambiÃ©n sobre los horarios de apertura del restaurante 'La Delicia'."
    )
    
    print("ğŸ› ï¸  Herramientas del mozo definidas: consultar_menu_y_horarios, off_topic_tool.")
    return [retriever_tool, off_topic_tool]

# --- 5. LÃ“GICA Y CONSTRUCCIÃ“N DEL GRAFO (AGENTE MOZO) ---

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]

def agent_node(state: AgentState, llm):
    """Invoca al LLM con el rol de mozo para que decida el siguiente paso."""
    system_prompt = """
    Eres "Bruno", el mozo virtual del restaurante "La Delicia". Eres amable, servicial y eficiente.
    Tu objetivo es ayudar a los clientes a conocer el menÃº y responder sus preguntas.

    Instrucciones:
    1.  Saluda al cliente y presÃ©ntate cordialmente.
    2.  Utiliza la herramienta `consultar_menu_y_horarios` para responder CUALQUIER pregunta sobre platos, ingredientes, precios, recomendaciones y horarios.
    3.  Si el cliente te pide una recomendaciÃ³n (ej. "algo liviano", "un plato sin carne"), usa la herramienta para buscar opciones y luego presÃ©ntalas de forma atractiva.
    4.  Si la pregunta no tiene NADA que ver con el restaurante, el menÃº o la comida, DEBES usar la herramienta `off_topic_tool`.
    5.  Basa tus respuestas ÃšNICAMENTE en la informaciÃ³n que te proporcionan tus herramientas. No inventes platos, precios ni horarios.
    6.  SÃ© conciso pero completo en tus respuestas. Si das un precio, menciÃ³nalo claramente.
    """
    messages = [SystemMessage(content=system_prompt)] + state["messages"]
    response = llm.invoke(messages)
    return {"messages": [response]}

def should_continue(state: AgentState) -> Literal["tools", "__end__"]:
    """Determina si se debe llamar a una herramienta o si el flujo ha terminado."""
    if state["messages"][-1].tool_calls:
        return "tools"
    return "__end__"

def build_graph(llm_with_tools, tools_list):
    """Construye y compila el grafo del agente mozo."""
    graph = StateGraph(AgentState)

    graph.add_node("agent", lambda state: agent_node(state, llm_with_tools))
    graph.add_node("tools", ToolNode(tools_list))

    graph.set_entry_point("agent")
    graph.add_conditional_edges(
        "agent", should_continue, {"tools": "tools", "__end__": END}
    )
    graph.add_edge("tools", "agent")

    print("ğŸ§  Grafo del mozo virtual construido y compilado.")
    return graph.compile()

# --- 6. EJECUCIÃ“N PRINCIPAL ---

if __name__ == "__main__":
    setup_environment()
    
    # ğŸ”§ Asegura que LangChain use la API key
    os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")
    
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)
    embedding_model = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

    documents = load_documents()
    vectorstore = create_or_load_vectorstore(documents, embedding_model)
    tools = define_tools(vectorstore)
    
    llm_with_tools = llm.bind_tools(tools)

    rag_agent = build_graph(llm_with_tools, tools)

     # MODIFICACIÃ“N: AÃ±adimos una lista para mantener el historial de la conversaciÃ³n.
    conversation_history = []
    
    print("\n\n" + "="*50)
    print("      ğŸ BIENVENIDO AL RESTAURANTE 'LA DELICIA' ğŸ")
    print("="*50)
    print("\nBruno, tu mozo virtual, estÃ¡ listo para atenderte.")
    print(" (Escribe 'salir' para terminar la conversaciÃ³n)")

    while True:
        query = input("\nğŸ‘¤ Cliente: ")
        if query.lower() in ["exit", "quit", "salir"]:
            print("\nğŸ‘‹ Bruno: Â¡Gracias por tu visita! Â¡Vuelve pronto!")
            break
        
        # Invocamos el agente con el historial completo MÃS la nueva pregunta
        # para que el agente tenga contexto de la conversaciÃ³n.

        conversation_history.append(HumanMessage(content=query))
        result = rag_agent.invoke({"messages": conversation_history})
    
        # La salida del grafo (`result`) contiene el estado final, que es la lista
        # completa de mensajes de la ejecuciÃ³n. La guardamos como nuestro nuevo historial.
        conversation_history = result["messages"]
        
        # La respuesta para el usuario es el contenido del Ãºltimo mensaje en el historial.
        final_response = conversation_history[-1].content
        print(f"\nğŸ¤– Bruno: {final_response}")
